{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ryans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "prediction is: 55.05714285714286\n",
      "0.3\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "prediction is: 55.528571428571425\n",
      "0.5\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "prediction is: 55.41428571428572\n",
      "0.7\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "prediction is: 54.92857142857142\n",
      "1\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "prediction is: 54.15714285714286\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "train_file = 'data_train.pkl'\n",
    "test_file  = 'data_test.pkl'\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    #load data and test sets\n",
    "    train = np.load(train_file, allow_pickle=True)\n",
    "    test  = np.load(test_file, allow_pickle=True)\n",
    "\n",
    "    inputs = np.asarray(train[0])\n",
    "    labels = np.asarray(train[1])\n",
    "\n",
    "    return inputs, labels, test\n",
    "\n",
    "\n",
    "class preprocessing():\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.excluded = [\"!\",\"#\",\"$\",\"%\",\"&\",\"''\",\"'\",\":\",\";\",\",\",\".\",\"(\",\")\",\n",
    "                \"|\",\"``\",\"[\",\"]\",\"...\",\"?\",\"\",\"*\",\"--\",\"*i\",\"s\",\n",
    "                \"'m\",\"'ll\",\"d\",\"'re\",\"n't\", \"'s\",\"'d\",\"'ve\",\"-\"]\n",
    "\n",
    "\n",
    "    #SOURCE: https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    #Get the POS tag for each word after tokenizing\n",
    "    def get_wordnet_pos(self, treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    #Split words using NLTK's tokenizer\n",
    "    def tokenize(self, inputs):\n",
    "        self.word_tokens = [list(nltk.pos_tag(word_tokenize(inputs[i]))) for i in range(len(inputs))]\n",
    "\n",
    "    #Word preprocessing:\n",
    "    #   Some words can be excluded (eg. punctuation marks),\n",
    "    #   We can also lemmatize or stem words\n",
    "    def filter_words(self, inputs,\n",
    "                     exclusion = False,\n",
    "                     lemmatize = False,\n",
    "                     stemmer = False):\n",
    "\n",
    "        self.new_inputs = np.array([None] * len(inputs))\n",
    "        self.tokenize(inputs)\n",
    "\n",
    "        if lemmatize : lemmatizer = WordNetLemmatizer()\n",
    "        if stemmer   : stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "        for i in range(len(inputs)):\n",
    "\n",
    "            temp = []\n",
    "            for j in range(len(self.word_tokens[i])):\n",
    "                #Lemmatizer\n",
    "                if lemmatize:\n",
    "                    words = lemmatizer.lemmatize(self.word_tokens[i][j][0],\n",
    "                                    self.get_wordnet_pos(self.word_tokens[i][j][1]))\n",
    "                #SnowballStemmer\n",
    "                if stemmer:\n",
    "                    words = stemmer.stem(self.word_tokens[i][j][0])\n",
    "\n",
    "                if not lemmatize and not stemmer:\n",
    "                    words = self.word_tokens[i][j][0]\n",
    "\n",
    "                #Cut stop words and excluded words\n",
    "                if exclusion:\n",
    "                    if words.lower() not in self.stop_words and words not in self.excluded:\n",
    "                        temp.append(words.lower())\n",
    "                #Cut only stop words\n",
    "                else:\n",
    "                    if words.lower() not in self.stop_words:\n",
    "                        temp.append(words.lower())\n",
    "            self.new_inputs[i] = temp\n",
    "\n",
    "        return self.new_inputs\n",
    "\n",
    "def data_split(train,k = None):\n",
    "\n",
    "    #on récupère les labels unique et on compte leurs nombres d'occurrences dans counts\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    train_ind = np.floor(0.9 * len(inputs)).astype('int32')\n",
    "    random.seed(5000)\n",
    "    indices = np.arange(len(train))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    if k is None:\n",
    "\n",
    "        train_indices = indices[:train_ind]\n",
    "        test_indices = indices[train_ind:]\n",
    "\n",
    "        train_inputs = inputs[train_indices]\n",
    "        test_inputs = inputs[test_indices]\n",
    "        train_labels = labels[train_indices]\n",
    "        test_labels = labels[test_indices]\n",
    "\n",
    "        x_train = train_inputs\n",
    "        y_train = train_labels\n",
    "        x_test  = test_inputs\n",
    "        y_test  = test_labels\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "#P( word = k| subreddit = j):\n",
    "class NBC():\n",
    "    def __init__(self, x_train,y_train, x_test, y_test,alpha, TFIDF = False,smoothing=True ):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.alpha = alpha\n",
    "        self.TFIDF = TFIDF\n",
    "        self.smoothing = smoothing\n",
    "        self.unique_labels, self.label_counts = np.unique(self.y_train, return_counts=True)\n",
    "        self.num_sentences = len(self.y_train)\n",
    "        self.len_subreddits = len(self.unique_labels)\n",
    "        self.the_sentence = np.concatenate([self.x_train[i] for i in range(len(self.x_train))])\n",
    "        self.words_all, self.counts_all = np.unique(self.the_sentence,return_counts=True)\n",
    "        self.vocab_size = len(self.words_all)\n",
    "        self.dict_labels = { self.unique_labels[i]: i for i in range(len(self.unique_labels)) }\n",
    "\n",
    "    #voir document Jurafsky - Naive Bayes sur Studium, slide 29\n",
    "    #regroupement des mots de tous les documents appartenant au meme label dans un seul array\n",
    "    def concatenated_sentence_by_subreddit(self):\n",
    "        #Concatenated sentence for each subreddit:\n",
    "        self.mega_sentence = np.array([None] * len(self.unique_labels))\n",
    "        self.n = np.array([None] * len(self.unique_labels))\n",
    "        #Number of unique words and their count in mega_sentence for each subreddit:\n",
    "        self.unique_words = np.array([None] * len(self.unique_labels))\n",
    "        self.unique_counts = np.array([None] * len(self.unique_labels))\n",
    "\n",
    "        for j in range(len(self.unique_labels)):\n",
    "            self.mega_sentence[j] = np.concatenate([self.x_train[i] for i in range(self.num_sentences) if self.y_train[i] == self.unique_labels[j] ]).flatten()\n",
    "            self.unique_words[j], self.unique_counts[j] = np.unique(self.mega_sentence[j],return_counts=True)\n",
    "            self.n[j] = len(self.mega_sentence[j])\n",
    "\n",
    "        self.k= self.vocab_size*self.alpha+self.n\n",
    "\n",
    "        return self.mega_sentence, self.unique_words, self.unique_counts, self.k\n",
    "\n",
    "\n",
    "    #P(word=k | subreddit=j)\n",
    "    def compute_probabilities(self):\n",
    "        self.concatenated_sentence_by_subreddit()\n",
    "\n",
    "        if self.TFIDF == False:\n",
    "            self.probabilities = { }\n",
    "            for i in range(len(self.words_all)):\n",
    "                self.probabilities[self.words_all[i]] = {}\n",
    "                for j in range(len(self.unique_labels)):\n",
    "                    ind = np.where( self.unique_words[j] == self.words_all[i] )\n",
    "\n",
    "                    #with Laplace smoothing\n",
    "                    if self.smoothing:\n",
    "                        if len(ind[0]) != 0 : self.probabilities[self.words_all[i]][self.unique_labels[j]] = np.log( (self.unique_counts[j][ind] + self.alpha)/self.k[j])\n",
    "                        if len(ind[0]) == 0: self.probabilities[self.words_all[i]][self.unique_labels[j]] = np.log( self.alpha/self.k[j])\n",
    "\n",
    "                    #Without smoothing\n",
    "                    else:\n",
    "                        if len(ind[0]) != 0 :\n",
    "                            self.probabilities[self.words_all[i]][self.unique_labels[j]] = np.log( (self.unique_counts[j][ind][0] )/ self.vocab_size+self.n[j])\n",
    "                        if len(ind[0]) == 0: self.probabilities[self.words_all[i]][self.unique_labels[j]] = 0.\n",
    "\n",
    "        if self.TFIDF :\n",
    "            #Creation of the dictionary for probabilities\n",
    "            self.probabilities = { }\n",
    "            #for each unique word among all documents:\n",
    "            for i in range(len(self.words_all)):\n",
    "                #create a dictionary for each word\n",
    "                self.probabilities[self.words_all[i]] = {}\n",
    "                #count in how many documents the word appears\n",
    "                count = 0\n",
    "                for j in range(len(self.unique_labels)):\n",
    "                    ind = np.where( self.unique_words[j] == self.words_all[i] )\n",
    "                    if len(ind[0]) != 0: count+=1\n",
    "\n",
    "                if count == 0: idf = np.log( self.len_subreddits )\n",
    "                elif count == self.len_subreddits: idf = np.log( self.len_subreddits / (self.len_subreddits - 0.5))\n",
    "                else: idf = np.log( self.len_subreddits / count )\n",
    "                #write all probabilities for the word for each subreddit:\n",
    "                for j in range(len(self.unique_labels)):\n",
    "                    ind = np.where( self.unique_words[j] == self.words_all[i] )\n",
    "\n",
    "                    #with Laplace smoothing\n",
    "                    if self.smoothing:\n",
    "\n",
    "                        if len(ind[0]) != 0 :\n",
    "                            self.probabilities[self.words_all[i]][self.unique_labels[j]] = np.log( ((self.unique_counts[j][ind] + self.alpha)/self.k[j]) * idf )\n",
    "                        if len(ind[0]) == 0: self.probabilities[self.words_all[i]][self.unique_labels[j]] = np.log( (self.alpha/self.k[j]) * idf)\n",
    "\n",
    "                    #Without smoothing\n",
    "                    else:\n",
    "                        if len(ind[0]) != 0 :\n",
    "                            self.probabilities[self.words_all[i]][self.unique_labels[j]] = np.log( ((self.unique_counts[j][ind][0] )/ self.vocab_size+self.n[j]) * idf )\n",
    "                        if len(ind[0]) == 0: self.probabilities[self.words_all[i]][self.unique_labels[j]] = 0.\n",
    "\n",
    "\n",
    "        return self.probabilities\n",
    "\n",
    "    def compute_predictions_sentence(self,sentence):\n",
    "\n",
    "        dicts = []\n",
    "        #smoothing\n",
    "        if self.smoothing:\n",
    "            if_empty = {self.unique_labels[j] : np.log(self.alpha / self.k[j] ) for j in range(len(self.unique_labels))}\n",
    "\n",
    "            for i in range(len(sentence)):\n",
    "\n",
    "                if sentence[i] in self.probabilities:\n",
    "                    dicts.append(self.probabilities[sentence[i]])\n",
    "                else:\n",
    "                    self.probabilities[sentence[i]] = if_empty\n",
    "                    dicts.append(self.probabilities[sentence[i]])\n",
    "            result = {k: sum(d[k] for d in dicts) + np.log(1./20.) for k in dicts[0].keys()}\n",
    "\n",
    "        #no smoothing\n",
    "        else:\n",
    "            if_empty = {self.unique_labels[j] : 0. for j in range(len(self.unique_labels))}\n",
    "            for i in range(len(sentence)):\n",
    "\n",
    "                if sentence[i] in self.probabilities:\n",
    "                    dicts.append(self.probabilities[sentence[i]])\n",
    "                else:\n",
    "                    self.probabilities[sentence[i]] = if_empty\n",
    "                    dicts.append(self.probabilities[sentence[i]])\n",
    "            result = {k: sum(d[k] for d in dicts) + np.log(1./20.) for k in dicts[0].keys()}\n",
    "\n",
    "        result = max(result, key=result.get)\n",
    "        return result\n",
    "\n",
    "    def compute_predictions(self):\n",
    "        success = []\n",
    "        preds = []\n",
    "        real = []\n",
    "        preds_to_write=[\"Category\"]\n",
    "        all_preds = []\n",
    "        ids=[]\n",
    "        ids.append(\"Id\")\n",
    "        ids.extend(list(range(30000)))\n",
    "\n",
    "        self.compute_probabilities()\n",
    "\n",
    "        for i in range(len(self.x_test)):\n",
    "            preds.append(self.compute_predictions_sentence(self.x_test[i]))\n",
    "#             if(i%100==0):\n",
    "#                 print(i)\n",
    "            real.append(self.y_test[i])\n",
    "\n",
    "        preds_to_write.extend(preds)\n",
    "#         with open(\"submission_TE_0.5.csv\", \"w\",newline='') as f:\n",
    "#                 writer = csv.writer(f)\n",
    "#                 writer.writerows(zip(ids,preds_to_write))\n",
    "\n",
    "        count=0\n",
    "        for i in range(len(preds)):\n",
    "            if preds[i]==real[i]:\n",
    "                count+=1\n",
    "        print(\"prediction is:\",100.*(count/len(preds)))\n",
    "#         return preds\n",
    "\n",
    "    def validation_error(self):\n",
    "\n",
    "        conf_mat = self.conf_matrix()\n",
    "        good_preds = np.sum(np.diag(conf_mat))\n",
    "        sum_preds = np.sum(conf_mat)\n",
    "\n",
    "        #a voir test error method\n",
    "        return 1.0 - good_preds/sum_preds\n",
    "\n",
    "    def conf_matrix(self):\n",
    "        n_classes = len(self.unique_labels)\n",
    "        matrix = np.zeros((n_classes,n_classes))\n",
    "        predlabels=self.compute_predictions()\n",
    "\n",
    "        for (test, pred) in zip(self.y_test, predlabels):\n",
    "            matrix[int(self.dict_labels[test]-1),int(self.dict_labels[pred]-1)] += 1\n",
    "\n",
    "        return matrix\n",
    "\n",
    "\n",
    "#Test\n",
    "k = None\n",
    "alpha = [0.1,0.3,0.5,0.7,1]\n",
    "inputs, labels, test = load_data()\n",
    "model_a = preprocessing()\n",
    "inputs = model_a.filter_words( inputs,\n",
    "                 exclusion = True,\n",
    "                 lemmatize = False,\n",
    "                 stemmer = False)\n",
    "test = model_a.filter_words( test,\n",
    "                 exclusion = True,\n",
    "                 lemmatize = False,\n",
    "                 stemmer = False)\n",
    "x_train, y_train, x_test, y_test= data_split(inputs, k)\n",
    "for a in alpha:\n",
    "    print(a)\n",
    "    nbc_model_a = NBC(x_train,y_train,x_test,y_test, a, TFIDF = False, smoothing=True)\n",
    "    nbc_model_a.compute_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

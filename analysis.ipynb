{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affecting-primary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing\n",
      "loading pkl\n",
      "preprocessing train set\n",
      "lower finished\n",
      "tokenize done\n",
      "join finished\n",
      "preprocessing test set\n",
      "lower finished\n",
      "tokenize done\n",
      "join finished\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Best hyper-parameters:\n",
      "{'voting': 'hard'}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocess import PreprocessNLP\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, ComplementNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import lightgbm as lgbm\n",
    "\n",
    "#Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import seaborn as sns\n",
    "import csv\n",
    "\n",
    "\n",
    "train_file = 'data_train.pkl'\n",
    "test_file  = 'data_test.pkl'\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "print(\"initializing\")\n",
    "\n",
    "\n",
    "print(\"loading pkl\")\n",
    "train = np.load(train_file, allow_pickle=True)\n",
    "test  = np.load(test_file, allow_pickle=True)\n",
    "inputs = np.asarray(train[0])\n",
    "labels = np.asarray(train[1])\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True,norm='l2',min_df=2, encoding='latin-1', strip_accents = 'unicode')\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'Messages': inputs,\n",
    "     'Labels': labels\n",
    "    })\n",
    "\n",
    "df_test = pd.DataFrame(\n",
    "    {'Messages': test,\n",
    "    })\n",
    "\n",
    "def prep(inputs_df, X):\n",
    "    _prep = PreprocessNLP( inputs_df, X )\n",
    "    _prep.lower()\n",
    "    _prep.tokenize()\n",
    "    _prep.join()\n",
    "\n",
    "    return _prep.data_frame\n",
    "\n",
    "print(\"preprocessing train set\")\n",
    "df = prep( df, 'Messages' )\n",
    "print(\"preprocessing test set\")\n",
    "df_test = prep( df_test, 'Messages')\n",
    "\n",
    "# for models:\n",
    "counts_tfidf = tfidf.fit_transform(df['Messages'])\n",
    "counts_tfidf_test = tfidf.transform(df_test['Messages'])\n",
    "\n",
    "\n",
    "#Some models from sklearn (after gridsearch)\n",
    "classifiers = [\n",
    "                # 'RandomForestClassifier',\n",
    "                'VotingClassifier',\n",
    "                # 'AdaBoostClassifier',\n",
    "                # 'ComplementNB',\n",
    "                # 'MultinomialNB',\n",
    "                # 'BernoulliNB',\n",
    "                # 'SGDClassifier',\n",
    "                ]\n",
    "models = {\n",
    "    # 'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'VotingClassifier': VotingClassifier(  estimators =  [\n",
    "                    ('cnb', ComplementNB(alpha=1.2)),\n",
    "                    ('mnb', MultinomialNB(alpha = 0.25) ),\n",
    "                    ('sgdc', SGDClassifier(loss = 'modified_huber', penalty = 'l2',\n",
    "                                            max_iter=1000, tol=1e-3,\n",
    "                                            learning_rate = 'optimal', random_state = 42 ) )\n",
    "                        ]),\n",
    "    # 'AdaBoostClassifier': AdaBoostClassifier(),\n",
    "    # 'ComplementNB': ComplementNB(),\n",
    "    # 'MultinomialNB': MultinomialNB(),\n",
    "    # 'BernoulliNB': BernoulliNB(),\n",
    "    # 'SGDClassifier': SGDClassifier(),\n",
    "\n",
    "}\n",
    "\n",
    "#best param grid\n",
    "params = {\n",
    "    # 'RandomForestClassifier': { 'n_estimators': [500], 'max_depth': [50] },  #resultat: 0.4295\n",
    "    'VotingClassifier' : {\n",
    "                        'voting': ['hard']\n",
    "                            },\n",
    "    # 'AdaBoostClassifier':  { 'base_estimator':\n",
    "    #                     [\n",
    "    #                     MultinomialNB(alpha=0.25),\n",
    "    #                     ],\n",
    "    #                     'n_estimators': [100], 'learning_rate': [1.0] },\n",
    "    # 'ComplementNB': { 'alpha': [1.55]},\n",
    "    # 'MultinomialNB': {'alpha' : [0.3]},\n",
    "    # 'BernoulliNB': {'alpha' : [0.3]}\n",
    "    # 'SGDClassifier':{ 'loss': ['modified_huber'], 'penalty': ['l2'], 'alpha' : [0.0001], \"random_state\": [42]}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#to compare them\n",
    "# X_train, X_test, y_train, y_test = train_test_split(counts_tfidf,\n",
    "#         df['Labels'], test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "preds = []\n",
    "clfs = classifiers\n",
    "\n",
    "for classifier in classifiers:\n",
    "    clf = GridSearchCV(models[classifier], params[classifier], cv=10,\n",
    "                            verbose = 1, scoring='accuracy')\n",
    "\n",
    "    clf.fit(counts_tfidf, df[\"Labels\"])\n",
    "\n",
    "    print(\"Best hyper-parameters:\")\n",
    "    print(clf.best_params_)\n",
    "\n",
    "    predicted = clf.predict(counts_tfidf_test)\n",
    "    # pred_acc = np.sum([y_test == y_preds]) / len(y_test)\n",
    "    # preds.append(pred_acc)\n",
    "\n",
    "# print(list(zip(classifiers, preds)))\n",
    "\n",
    "#Keras stuff\n",
    "# df = pd.DataFrame(\n",
    "#     {'Messages': inputs,\n",
    "#      'Labels': labels\n",
    "#     })\n",
    "#\n",
    "# df_test = pd.DataFrame(\n",
    "#     {'Messages': test,\n",
    "#     })\n",
    "#\n",
    "#\n",
    "# one_hot = pd.get_dummies(df[\"Labels\"])\n",
    "# df.drop(['Labels'],axis=1,inplace=True)\n",
    "# df = pd.concat([df,one_hot],axis=1)\n",
    "#\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df[\"Messages\"].values,\n",
    "#         df.drop(['Messages'],axis=1).values, test_size=0.1, random_state=42)\n",
    "#\n",
    "#\n",
    "#\n",
    "# epochs = 10\n",
    "\n",
    "\n",
    "#Transform inputs with keras Tokenizer\n",
    "# num_words = 20000\n",
    "# tokenizer = Tokenizer(num_words= num_words)\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "# sequences = tokenizer.texts_to_sequences(X_train)\n",
    "# num_words = len(tokenizer.word_index) + 1\n",
    "# X_train = pad_sequences(sequences, maxlen=200)\n",
    "# sequences = tokenizer.texts_to_sequences(X_test)\n",
    "# X_test = pad_sequences(sequences, maxlen=200)\n",
    "#\n",
    "#\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(num_words, 50, input_length=200))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(20))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Activation('softmax'))\n",
    "#\n",
    "# callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "#          ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                     batch_size=64,\n",
    "#                     epochs=epochs,\n",
    "#                     verbose=1,\n",
    "#                     validation_split=0.1)\n",
    "# fig = plt.figure()\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Précision du réseau de neurones en fonction de l\\'époque')\n",
    "# plt.ylabel('Précision')\n",
    "# plt.xlabel('Époque')\n",
    "# ax = fig.gca()\n",
    "# ax.set_xticks(np.arange(0, epochs, 1.))\n",
    "# ax.set_yticks(np.arange(0, np.ceil(max(history.history['val_accuracy'])), 0.1))\n",
    "# plt.grid()\n",
    "# plt.legend(['Entraînement', 'Test'], loc='upper left')\n",
    "# plt.show()\n",
    "#\n",
    "# fig = plt.figure()\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('Perte du réseau de neurones en fonction de l\\'époque')\n",
    "# plt.ylabel('Perte')\n",
    "# plt.xlabel('Époque')\n",
    "# ax = fig.gca()\n",
    "# ax.set_xticks(np.arange(0, epochs, 1.))\n",
    "# ax.set_yticks(np.arange(0, np.ceil(max(max(history.history['loss']), max(history.history['val_loss']))), 0.5))\n",
    "# plt.grid()\n",
    "# plt.legend(['Entraînement', 'Test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# score, acc = model.evaluate(X_test, y_test,\n",
    "#                             batch_size=64)\n",
    "# print(score, acc)\n",
    "\n",
    "\n",
    " ####Creating the csv submission\n",
    "preds = []\n",
    "preds_to_write=[\"Category\"]\n",
    "all_preds = []\n",
    "ids=[]\n",
    "ids.append(\"Id\")\n",
    "ids.extend(list(range(30000)))\n",
    "\n",
    "for i in predicted:\n",
    "    preds.append(i)\n",
    "\n",
    "preds_to_write.extend(preds)\n",
    "\n",
    "with open(\"predictions.csv\", \"w\",newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(zip(ids,preds_to_write))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-native",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-package",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
